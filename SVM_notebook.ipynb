{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wang_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\wang_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\wang_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\wang_\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read complete\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(500)\n",
    "# insert path to csv file\n",
    "Corpusi = pd.read_csv(r\"D:\\Cam_Y2\\Group project\\Question classifier\\test_csv.csv\",encoding='latin-1')\n",
    "Corpusi = pd.read_csv(r\"D:\\Cam_Y2\\Group project\\Question classifier\\version1.csv\",encoding='latin-1',low_memory=False)\n",
    "print(\"read complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text           label  \\\n",
      "0           When did Beyonce start becoming popular?  conversational   \n",
      "1  What areas did Beyonce compete in when she was...  conversational   \n",
      "2  When did Beyonce leave Destiny's Child and bec...  conversational   \n",
      "3      In what city and state did Beyonce  grow up?   conversational   \n",
      "4         In which decade did Beyonce become famous?  conversational   \n",
      "\n",
      "  Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7  \n",
      "0        NaN        NaN        NaN        NaN        NaN        NaN  \n",
      "1        NaN        NaN        NaN        NaN        NaN        NaN  \n",
      "2        NaN        NaN        NaN        NaN        NaN        NaN  \n",
      "3        NaN        NaN        NaN        NaN        NaN        NaN  \n",
      "4        NaN        NaN        NaN        NaN        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "Corpus = Corpusi.head(1500)\n",
    "print(Corpus.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "                                                text           label  \\\n",
      "0  When was a second public service held for Prin...  conversational   \n",
      "1               What is the name of Queen's drummer?  conversational   \n",
      "2  How many games has Everton played against Asto...  conversational   \n",
      "3                    To what is Sanskrit restricted?  conversational   \n",
      "4  What is one reasons Shakespeare is a good plac...  conversational   \n",
      "\n",
      "  Unnamed: 2 Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7  \n",
      "0        NaN        NaN        NaN        NaN        NaN        NaN  \n",
      "1        NaN        NaN        NaN        NaN        NaN        NaN  \n",
      "2        NaN        NaN        NaN        NaN        NaN        NaN  \n",
      "3        NaN        NaN        NaN        NaN        NaN        NaN  \n",
      "4        NaN        NaN        NaN        NaN        NaN        NaN  \n"
     ]
    }
   ],
   "source": [
    "dtype = {'text': str,'label': str}\n",
    "dirty_data = Corpusi\n",
    "row_count = len(dirty_data.index)\n",
    "med_data = dirty_data[dirty_data[\"label\"] == 'medical']\n",
    "dirty_data = dirty_data.sample(n = 1500)\n",
    "dirty_data = dirty_data[dirty_data[\"label\"] == 'conversational']\n",
    "new_data = pd.concat([dirty_data,med_data])\n",
    "Corpus = new_data.sample(frac = 1).reset_index(drop=True)\n",
    "is_list = type(Corpus['text'].head().iloc[0])\n",
    "print(is_list)\n",
    "print(Corpus.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step a-c complete\n",
      "0    ['second', 'public', 'service', 'hold', 'princ...\n",
      "1                         ['name', 'queen', 'drummer']\n",
      "2    ['many', 'game', 'everton', 'play', 'aston', '...\n",
      "3                             ['sanskrit', 'restrict']\n",
      "4    ['one', 'reason', 'shakespeare', 'good', 'plac...\n",
      "Name: text_final, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Step - a : Remove blank rows if any.\n",
    "Corpus['text'].dropna(inplace=True)\n",
    "\n",
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "Corpus['text'] = [entry.lower() for entry in Corpus['text']]\n",
    "# (Alex) Website said: Corpus['text'] = [entry.str.lower() for entry in Corpus['text']]\n",
    "\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "Corpus['text']= [word_tokenize(entry) for entry in Corpus['text']]\n",
    "# (Alex) \n",
    "# I'm not really sure what is going on here, but the imported data is already in tocken form. The series Corpus['text'] \n",
    "# contains lists of words, which is essentially the tockenised form. Supposedly, the author of the webpage got a string \n",
    "# as each entry in Corpus['text'], thus he used \".lower()\" on the entry, then try to tockenise it.\n",
    "# Finally, the above prosedure does not take signifigantly more time, plus this should be done at configure time, so it should be fine\n",
    "\n",
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "\n",
    "print(\"step a-c complete\")\n",
    "\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(Corpus['text']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    Corpus.loc[index,'text_final'] = str(Final_words)\n",
    "\n",
    "print(Corpus['text_final'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X, Test_X, Train_Y, Test_Y = model_selection.train_test_split(Corpus['text_final'],Corpus['label'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    ['second', 'public', 'service', 'hold', 'princ...\n",
      "1                         ['name', 'queen', 'drummer']\n",
      "2    ['many', 'game', 'everton', 'play', 'aston', '...\n",
      "3                             ['sanskrit', 'restrict']\n",
      "4    ['one', 'reason', 'shakespeare', 'good', 'plac...\n",
      "Name: text_final, dtype: object\n"
     ]
    }
   ],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Y = Encoder.fit_transform(Train_Y)\n",
    "Test_Y = Encoder.fit_transform(Test_Y)\n",
    "print(Corpus['text_final'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(Corpus['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Tfidf_vect.vocabulary_)\n",
    "#print(Train_X_Tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  92.19269102990033\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)\n",
    "# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13fdf25ef2beee923e1f1011635d38d6ce68fb2882c6a64f76de1420ab9cc9f4"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
